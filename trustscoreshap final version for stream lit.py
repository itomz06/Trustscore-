# -*- coding: utf-8 -*-
"""TrustscoreShap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-Qmkz2NRiyCbJ31nTmTjZIabPsq926b
"""

# -*- coding: utf-8 -*-
"""TrustscoreShap.ipynb - Fixed Version for Google Colab

Automatically generated by Colab with enhanced SHAP explanations.

Original file is located at
    https://colab.research.google.com/drive/1m-Qmkz2NRiyCbJ31nTmTjZIabPsq926b
"""

# ========== ğŸ” IMPORTS ========== #
import pandas as pd
import joblib
import numpy as np
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from google.colab import files

from trustscore_batch_analyzer import TrustScoreAnalyzer, create_sample_data

# ========== ğŸ› ï¸ GENERATE DATA ========== #
df = create_sample_data(5000)
print(f"âœ… Generated dataset with shape: {df.shape}")
df.to_csv("generated_trustscore_data.csv", index=False)
print("ğŸ’¾ Dataset saved to generated_trustscore_data.csv")

# ========== ğŸ” RUN TRUSTSCORE ANALYZER ========== #
analyzer = TrustScoreAnalyzer()
results_df = analyzer.analyze_batch(df)
summary = analyzer.generate_summary_report(results_df)

print("\n=== ğŸ” ANALYSIS SUMMARY ===")
print(f"ğŸ“Š Total Records Processed: {summary['total_records']}")
print(f"âœ… Overall Approval Rate: {summary['approval_rate']:.1f}%")
print(f"âš ï¸ High Risk Applications: {summary['high_risk_count']}")

print("\n=== ğŸ“‚ DECISION BREAKDOWN ===")
for decision, count in summary['decision_distribution'].items():
    percentage = summary['decision_percentages'][decision]
    print(f"  {decision}: {count} ({percentage}%)")

print("\n=== ğŸ“ˆ SCORE STATISTICS ===")
score_stats = summary['score_statistics']['overall_score']
print(f"  Average Score: {score_stats['mean']:.1f}")
print(f"  Score Range: {score_stats['min']:.0f} - {score_stats['max']:.0f}")

print("\nTop 5 Highest Scores:")
display(results_df.nlargest(5, 'overall_score')[['overall_score', 'decision', 'risk_category', 'income', 'credit_score']])

print("Top 5 Lowest Scores:")
display(results_df.nsmallest(5, 'overall_score')[['overall_score', 'decision', 'risk_category', 'income', 'credit_score']])

print("\nğŸ“Š Generating visualizations...")
analyzer.create_visualizations(results_df)

results_df.to_csv("trustscore_results_5000.csv", index=False)
print("ğŸ’¾ Results saved to trustscore_results_5000.csv")
files.download("trustscore_results_5000.csv")

# ========== ğŸ¤– TRAIN MODEL ========== #
features = [
    'income', 'credit_score', 'loan_amount', 'loan_term', 'age', 'monthly_debt',
    'employment', 'loan_purpose', 'gender', 'residence'
]

X = pd.get_dummies(results_df[features], drop_first=True)
y = results_df['decision']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train, y_train)

print("âœ… Model Training Complete!")
y_pred = rf_model.predict(X_test)
print("\nğŸ“Š Classification Report:")
print(classification_report(y_test, y_pred))

print("\nğŸ“‰ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# ========== ğŸ’¾ EXPORT MODEL ========== #
model_path = "trustscore_rf_model_5000.pkl"
joblib.dump(rf_model, model_path)
print(f"ğŸ’¾ Model saved to {model_path}")
files.download(model_path)

# ========== ğŸ”® SAMPLE PREDICTIONS ========== #
df_new = create_sample_data(5)
df_new_encoded = pd.get_dummies(df_new[features], drop_first=True)
df_new_encoded = df_new_encoded.reindex(columns=X.columns, fill_value=0)
new_predictions = rf_model.predict(df_new_encoded)
df_new['predicted_decision'] = new_predictions
print("\nğŸ”® New Predictions:")
print(df_new[['income', 'credit_score', 'loan_amount', 'employment', 'loan_purpose', 'predicted_decision']])

# ========== ğŸ” ENHANCED SHAP EXPLAINABILITY ========== #
print("\n" + "="*60)
print("ğŸ” SHAP ANALYSIS - DECISION EXPLANATIONS")
print("="*60)

try:
    # Initialize SHAP for Jupyter/Colab
    shap.initjs()

    # Select samples for explanation (mix of approved/rejected)
    approved_samples = X_test[y_test == 'Approved'].head(3)
    rejected_samples = X_test[y_test == 'Rejected'].head(2)
    sample_data = pd.concat([approved_samples, rejected_samples])
    sample_labels = ['Approved'] * len(approved_samples) + ['Rejected'] * len(rejected_samples)

    print(f"ğŸ“Š Analyzing {len(sample_data)} sample decisions...")

    # Create SHAP explainer
    explainer = shap.TreeExplainer(rf_model)
    shap_values = explainer.shap_values(sample_data)

    # Get class names and determine target class index
    classes = rf_model.classes_
    print(f"ğŸ¯ Model Classes: {classes}")

    # For binary classification, we'll focus on the 'Approved' class
    if len(shap_values) == 2:
        target_class_idx = list(classes).index('Approved') if 'Approved' in classes else 0
        target_shap_values = shap_values[target_class_idx]
        target_class = classes[target_class_idx]
    else:
        target_shap_values = shap_values
        target_class = 'Positive'

    print(f"ğŸ¯ Explaining decisions for class: {target_class}")

    # ========== DETAILED WRITTEN EXPLANATIONS ========== #
    def explain_decision(sample_idx, sample_row, shap_row, prediction, actual_label):
        """Generate human-readable explanation for a single decision"""

        print(f"\n{'='*50}")
        print(f"ğŸ“‹ DECISION EXPLANATION #{sample_idx + 1}")
        print(f"{'='*50}")

        # Basic info
        print(f"ğŸ¯ Predicted: {prediction}")
        print(f"âœ… Actual: {actual_label}")
        print(f"ğŸ² Confidence: {rf_model.predict_proba([sample_row])[0].max():.1%}")

        # Get feature impacts
        feature_impacts = [(col, val, shap_val) for col, val, shap_val in
                          zip(sample_data.columns, sample_row, shap_row)]
        feature_impacts.sort(key=lambda x: abs(x[2]), reverse=True)

        # Show top contributing factors
        print(f"\nğŸ” TOP FACTORS INFLUENCING THIS DECISION:")
        print("-" * 45)

        for i, (feature, value, impact) in enumerate(feature_impacts[:5]):
            if abs(impact) < 0.001:  # Skip very small impacts
                continue

            # Determine impact direction and strength
            if impact > 0:
                direction = "SUPPORTS APPROVAL"
                arrow = "â†—ï¸"
                color = "ğŸŸ¢"
            else:
                direction = "SUPPORTS REJECTION"
                arrow = "â†˜ï¸"
                color = "ğŸ”´"

            strength = "Strong" if abs(impact) > 0.1 else "Moderate" if abs(impact) > 0.05 else "Weak"

            print(f"{i+1}. {color} {feature}")
            print(f"   Value: {value}")
            print(f"   Impact: {strength} {direction} {arrow}")
            print(f"   Score: {impact:+.4f}")
            print()

        # Risk assessment
        risk_factors = [f for f, v, i in feature_impacts if i < -0.05]
        positive_factors = [f for f, v, i in feature_impacts if i > 0.05]

        print(f"âš ï¸  RISK FACTORS ({len(risk_factors)}): {', '.join(risk_factors[:3])}")
        print(f"âœ… POSITIVE FACTORS ({len(positive_factors)}): {', '.join(positive_factors[:3])}")

        # Decision summary
        base_score = explainer.expected_value[target_class_idx] if len(shap_values) == 2 else explainer.expected_value
        final_score = base_score + shap_row.sum()

        print(f"\nğŸ“Š SCORING BREAKDOWN:")
        print(f"   Base Score: {base_score:.4f}")
        print(f"   Feature Adjustments: {shap_row.sum():+.4f}")
        print(f"   Final Score: {final_score:.4f}")

        # Recommendation
        if prediction == 'Approved':
            print(f"\nğŸ’¡ DECISION RATIONALE:")
            print(f"   This application was APPROVED because the positive factors")
            print(f"   (particularly {positive_factors[0] if positive_factors else 'overall profile'})")
            print(f"   outweighed any risk concerns.")
        else:
            print(f"\nğŸ’¡ DECISION RATIONALE:")
            print(f"   This application was REJECTED due to significant risk factors")
            print(f"   (especially {risk_factors[0] if risk_factors else 'overall risk profile'})")
            print(f"   that outweighed positive aspects.")

    # Generate explanations for each sample
    predictions = rf_model.predict(sample_data)

    for i in range(len(sample_data)):
        explain_decision(
            i,
            sample_data.iloc[i],
            target_shap_values[i],
            predictions[i],
            sample_labels[i]
        )

    # ========== FEATURE IMPORTANCE SUMMARY ========== #
    print(f"\n{'='*60}")
    print("ğŸ“ˆ OVERALL FEATURE IMPORTANCE ANALYSIS")
    print("="*60)

    # Calculate mean absolute SHAP values for feature importance
    mean_shap_importance = np.abs(target_shap_values).mean(axis=0)
    feature_importance_df = pd.DataFrame({
        'Feature': sample_data.columns,
        'Importance': mean_shap_importance
    }).sort_values('Importance', ascending=False)

    print("\nğŸ† TOP 10 MOST INFLUENTIAL FEATURES:")
    print("-" * 40)
    for i, row in feature_importance_df.head(10).iterrows():
        print(f"{i+1:2d}. {row['Feature']:25s} | {row['Importance']:.4f}")

    # ========== SAVE EXPLANATIONS ========== #
    # Save detailed explanations to file
    explanation_data = []
    for i in range(len(sample_data)):
        explanation_data.append({
            'sample_id': i+1,
            'prediction': predictions[i],
            'actual': sample_labels[i],
            'confidence': rf_model.predict_proba([sample_data.iloc[i]])[0].max(),
            'top_positive_factor': feature_importance_df.iloc[0]['Feature'],
            'shap_explanation': f"Sample {i+1}: {predictions[i]} decision"
        })

    explanation_df = pd.DataFrame(explanation_data)
    explanation_df.to_csv("trustscore_shap_explanations.csv", index=False)
    print(f"\nğŸ’¾ Detailed explanations saved to: trustscore_shap_explanations.csv")
    files.download("trustscore_shap_explanations.csv")

    # ========== VISUALIZATIONS ========== #
    print(f"\n{'='*60}")
    print("ğŸ“Š GENERATING SHAP VISUALIZATIONS")
    print("="*60)

    # 1. SHAP Summary Plot
    try:
        print("\nğŸ“ˆ Creating SHAP Summary Plot...")
        plt.figure(figsize=(10, 8))
        shap.summary_plot(target_shap_values, sample_data,
                         feature_names=sample_data.columns.tolist(),
                         show=False, max_display=15)
        plt.title('SHAP Feature Importance Summary', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('shap_summary_plot.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("âœ… SHAP Summary Plot saved as 'shap_summary_plot.png'")
        files.download('shap_summary_plot.png')
    except Exception as e:
        print(f"âš ï¸ Could not create SHAP summary plot: {e}")

    # 2. SHAP Feature Importance Bar Plot
    try:
        print("\nğŸ“Š Creating SHAP Feature Importance Bar Plot...")
        plt.figure(figsize=(10, 8))
        shap.plots.bar(target_shap_values, max_display=15, show=False)
        plt.title('SHAP Feature Importance (Mean |SHAP Value|)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('shap_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("âœ… SHAP Feature Importance Plot saved as 'shap_feature_importance.png'")
        files.download('shap_feature_importance.png')
    except Exception as e:
        # Fallback: Create custom feature importance plot
        print(f"âš ï¸ Using fallback feature importance plot: {e}")
        plt.figure(figsize=(10, 8))
        top_features = feature_importance_df.head(15)
        plt.barh(range(len(top_features)), top_features['Importance'].values)
        plt.yticks(range(len(top_features)), top_features['Feature'].values)
        plt.xlabel('Mean |SHAP Value|')
        plt.title('SHAP Feature Importance (Top 15 Features)', fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig('shap_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.show()
        files.download('shap_feature_importance.png')

    # 3. Individual Force Plots (for first 3 samples)
    try:
        print("\nğŸ” Creating Individual SHAP Force Plots...")
        for i in range(min(3, len(sample_data))):
            plt.figure(figsize=(12, 3))
            shap.plots.waterfall(
                explainer.expected_value[target_class_idx] if len(shap_values) == 2 else explainer.expected_value,
                target_shap_values[i],
                feature_names=sample_data.columns.tolist(),
                max_display=10,
                show=False
            )
            plt.title(f'SHAP Waterfall Plot - Sample {i+1} ({predictions[i]})',
                     fontsize=12, fontweight='bold')
            plt.tight_layout()
            plt.savefig(f'shap_waterfall_sample_{i+1}.png', dpi=300, bbox_inches='tight')
            plt.show()
        print(f"âœ… Created {min(3, len(sample_data))} individual SHAP waterfall plots")
        for i in range(min(3, len(sample_data))):
            files.download(f'shap_waterfall_sample_{i+1}.png')
    except Exception as e:
        print(f"âš ï¸ Could not create waterfall plots: {e}")

    print(f"\nâœ… SHAP Analysis Complete! Generated explanations for {len(sample_data)} decisions.")

except Exception as e:
    print(f"âŒ SHAP Analysis Error: {str(e)}")
    print("ğŸ“ Falling back to basic feature importance...")

    # Fallback: Use feature importance from Random Forest
    feature_names = X.columns
    importances = rf_model.feature_importances_

    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)

    print("\nğŸ† TOP 10 FEATURES (Random Forest Importance):")
    for i, row in importance_df.head(10).iterrows():
        print(f"{i+1:2d}. {row['feature']:25s} | {row['importance']:.4f}")

# ========== ENHANCED CONFUSION MATRIX VISUALIZATION ========== #
print(f"\n{'='*60}")
print("ğŸ“Š ENHANCED CONFUSION MATRIX ANALYSIS")
print("="*60)

try:
    from sklearn.metrics import ConfusionMatrixDisplay
    import seaborn as sns

    # Create enhanced confusion matrix
    plt.figure(figsize=(12, 5))

    # Subplot 1: Standard confusion matrix
    plt.subplot(1, 2, 1)
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)
    disp.plot(ax=plt.gca(), cmap='Blues', values_format='d')
    plt.title('Confusion Matrix\n(Raw Counts)', fontweight='bold')

    # Subplot 2: Normalized confusion matrix
    plt.subplot(1, 2, 2)
    cm_normalized = confusion_matrix(y_test, y_pred, normalize='true')
    disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=rf_model.classes_)
    disp_norm.plot(ax=plt.gca(), cmap='Blues', values_format='.2%')
    plt.title('Normalized Confusion Matrix\n(Percentages)', fontweight='bold')

    plt.tight_layout()
    plt.savefig('enhanced_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Calculate detailed metrics
    tn, fp, fn, tp = cm.ravel() if len(cm.ravel()) == 4 else (0, 0, 0, len(y_test))

    print("\nğŸ“Š DETAILED PERFORMANCE METRICS:")
    print("-" * 40)
    print(f"True Positives (Approved correctly):  {tp:4d}")
    print(f"True Negatives (Rejected correctly):  {tn:4d}")
    print(f"False Positives (Wrongly approved):   {fp:4d}")
    print(f"False Negatives (Wrongly rejected):   {fn:4d}")
    print("-" * 40)
    print(f"Accuracy:     {(tp + tn) / (tp + tn + fp + fn):.3f}")
    print(f"Precision:    {tp / (tp + fp) if (tp + fp) > 0 else 0:.3f}")
    print(f"Recall:       {tp / (tp + fn) if (tp + fn) > 0 else 0:.3f}")
    print(f"Specificity:  {tn / (tn + fp) if (tn + fp) > 0 else 0:.3f}")

    # Business impact analysis
    print(f"\nğŸ’¼ BUSINESS IMPACT ANALYSIS:")
    print("-" * 40)
    print(f"ğŸ“ˆ Applications Correctly Approved: {tp}")
    print(f"ğŸ“‰ Applications Correctly Rejected: {tn}")
    print(f"âš ï¸  Risk: Wrongly Approved (Potential Defaults): {fp}")
    print(f"ğŸ’” Missed Opportunity (Good Apps Rejected): {fn}")

    approval_rate_actual = (tp + fp) / len(y_test)
    approval_rate_optimal = (tp + fn) / len(y_test)
    print(f"ğŸ“Š Current Approval Rate: {approval_rate_actual:.1%}")
    print(f"ğŸ¯ Optimal Approval Rate: {approval_rate_optimal:.1%}")

    print("âœ… Enhanced confusion matrix saved as 'enhanced_confusion_matrix.png'")
    files.download('enhanced_confusion_matrix.png')

except Exception as e:
    print(f"âš ï¸ Could not create enhanced confusion matrix: {e}")
    # Fallback to basic confusion matrix
    print("\nğŸ“Š Basic Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

# ========== MODEL PERFORMANCE SUMMARY ========== #
print(f"\n{'='*60}")
print("ğŸ“ˆ FINAL MODEL PERFORMANCE SUMMARY")
print("="*60)

try:
    # Feature importance comparison
    rf_importance = pd.DataFrame({
        'Feature': X.columns,
        'RF_Importance': rf_model.feature_importances_
    }).sort_values('RF_Importance', ascending=False)

    print("\nğŸ† TOP 10 MOST IMPORTANT FEATURES:")
    print("   (Random Forest Feature Importance)")
    print("-" * 50)
    for i, row in rf_importance.head(10).iterrows():
        print(f"{i+1:2d}. {row['Feature']:25s} | {row['RF_Importance']:.4f}")

    # Save comprehensive results
    performance_summary = {
        'model_type': 'Random Forest Classifier',
        'training_samples': len(X_train),
        'test_samples': len(X_test),
        'features_used': len(X.columns),
        'accuracy': (tp + tn) / (tp + tn + fp + fn) if 'tp' in locals() else 'N/A',
        'approval_rate': approval_rate_actual if 'approval_rate_actual' in locals() else 'N/A'
    }

    print(f"\nğŸ“‹ MODEL CONFIGURATION:")
    print("-" * 30)
    for key, value in performance_summary.items():
        print(f"{key.replace('_', ' ').title()}: {value}")

except Exception as e:
    print(f"âš ï¸ Could not generate final summary: {e}")

print(f"\nğŸ‰ COMPLETE TRUSTSCORE ANALYSIS WITH VISUALIZATIONS FINISHED!")
print("="*60)
print("ğŸ“ Generated Files:")
print("  â€¢ trustscore_results_5000.csv")
print("  â€¢ trustscore_rf_model_5000.pkl")
print("  â€¢ trustscore_shap_explanations.csv")
print("  â€¢ shap_summary_plot.png")
print("  â€¢ shap_feature_importance.png")
print("  â€¢ shap_waterfall_sample_*.png")
print("  â€¢ enhanced_confusion_matrix.png")
print("="*60)
